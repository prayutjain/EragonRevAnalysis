# CRO Revenue Intelligence â€“ README

Welcome to the **Enterprise CRO Analytics System**, an AIâ€‘powered platform that lets Chief Revenue Officers ask plainâ€‘English questions and receive dataâ€‘driven answers, complete with visualisations and citations.

---

## âœ¨ Key Features

* **Naturalâ€‘language Q\&A** over revenue data
* **Triâ€‘store data model** (DuckDBÂ +Â Neo4jÂ +Â Chroma) for speed, relationships, and semantic fallback
* **Autoâ€‘discovery ingestion**â€”just drop your CSVs, restart, and query
* **Streaming API** for nearâ€‘instant feedback in the UI
* **Executive formatting** with optional charts & confidence indicators

---

## ğŸš€ Quickâ€¯Start (Local Dev)

```bash
# 1. Clone
$ git clone https://github.com/yourâ€‘org/croâ€‘analytics.git && cd croâ€‘analytics

# 2. Backâ€‘end
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt
$ cp .env.example .env                    # add your OpenAI key etc.
$ python backend/cro_api_server.py        # default port 8083

# 3. Frontâ€‘end (optional)
$ cd frontend
$ npm install && npm run dev      # http://localhost:3000
```

\###â€¯Prerequisites

| Tool                  | Version               |
| --------------------- | --------------------- |
| Python                | â‰¥Â 3.12                |
| NodeÂ /Â npm            | â‰¥Â 20                  |
| DuckDB                | bundled as Python lib |
| Neo4j DesktopÂ orÂ Aura | 5.x                   |

> **Dockerâ€‘first?** A `docker-compose.yml` spins up API, Neo4j, and Chroma with one command: `docker compose up`.

---

## ğŸ—‚ï¸Â ProjectÂ Structure

```
.
â”œâ”€ backend/
â”‚   â”œâ”€ ingestion/
â”‚   â”‚   â”œâ”€ data_ingestion.py    # KG and Embedding constructor
â”‚   â”‚   â””â”€ schema_discovery.py  # Data Schema builder - for LLM context
â”‚   â”œâ”€ chat_engines/
â”‚   â”‚   â”œâ”€ query_engine.py      # orchestrator
â”‚   â”‚   â””â”€ cro_query_engine.py  # executive formatter & viz helper
â”‚   â”œâ”€ cro_api_server.py        # FastAPI gateway
â”‚   â””â”€ components/              # visualisers, pydantic models, utils
â”‚   â”‚   â”œâ”€ nodes.py             # core planner/retriever/reasoner
â”‚   â”‚   â””â”€ tools.py             # node tools
â”‚   â”‚   â””â”€ models.py            # pydantic models
â”‚   â”œâ”€ data/                    # drop CSVs here
â”‚   â”œâ”€ frontend/                # React/Next.js UI
â”‚   â”œâ”€ config.py                # COnfiguration + keys
â”‚   â”œâ”€ TechDoc.md               # inâ€‘depth engine architecture
â”‚   â”œâ”€ SetupGuide.md            # Setup doc
â”‚   â”œâ”€ README.md
â”œâ”€ frontend/
â”‚   â”œâ”€ pages/
â”‚   â”‚   â”œâ”€ index.js             # Main frontend monolith
â”‚   â”‚   â””â”€ AnalyticsDashboard.js# Analytics component
â”‚   â”œâ”€ styles/
                   
```

---

## âš™ï¸Â Configuration

| Variable                                    | Purpose                |
| ------------------------------------------- | ---------------------- |
| `OPENAI_API_KEY`                            | LLM access             |
| `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` | Graph store            |
| `CHROMA_PATH`                               | Vector DB path         |
| `DATA_PATH`                                 | Folder containing CSVs |

All variables live in `.env` and are loaded by `pythonâ€‘dotenv`.

---

## ğŸ§ªÂ Testing

```bash
$ pytest backend/test_queries          # unit tests for search engine
$ python database_diagnosis.py         # Test embeddings, SQL, KG properties
```

---

## ğŸ› ï¸Â Extending

* **New data source?** Drop the file in `data/` and restart; the ingestion script rebuilds DuckDB tables, graph nodes, and embeddings.
* **Swap storage** layers (e.g., Snowflake for DuckDB) by editing `config.yaml`â€”the LLM prompt remains unchanged.
* **Custom tools** can be added to `ToolExecutor`; register in the Planner node to enable autoâ€‘selection.

