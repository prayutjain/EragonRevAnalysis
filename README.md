# CRO Revenue Intelligence â€“ README

Welcome to the **Enterprise CRO Analytics System**, an AIâ€‘powered platform that lets Chief Revenue Officers ask plainâ€‘English questions and receive dataâ€‘driven answers, complete with visualisations and citations.

---

## âœ¨ Key Features

* **Naturalâ€‘language Q\&A** over revenue data
* **Triâ€‘store data model** (DuckDBÂ +Â Neo4jÂ +Â Chroma) for speed, relationships, and semantic fallback
* **Autoâ€‘discovery ingestion**â€”just drop your CSVs, restart, and query
* **Streaming API** for nearâ€‘instant feedback in the UI
* **Executive formatting** with optional charts & confidence indicators

---

## ğŸš€ Quickâ€¯Start (Local Dev)

```bash
# 1. Clone
$ git clone https://github.com/yourâ€‘org/croâ€‘analytics.git && cd croâ€‘analytics

# 2. Backâ€‘end
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt
$ cp .env.example .env            # add your OpenAI key etc.
$ python src/api_server.py        # default port 8083

# 3. Frontâ€‘end (optional)
$ cd frontend
$ npm install && npm run dev      # http://localhost:3000
```

\###â€¯Prerequisites

| Tool                  | Version               |
| --------------------- | --------------------- |
| Python                | â‰¥Â 3.12                |
| NodeÂ /Â npm            | â‰¥Â 20                  |
| DuckDB                | bundled as Python lib |
| Neo4j DesktopÂ orÂ Aura | 5.x                   |

> **Dockerâ€‘first?** A `docker-compose.yml` spins up API, Neo4j, and Chroma with one command: `docker compose up`.

---

## ğŸ—‚ï¸Â ProjectÂ Structure

```
.
â”œâ”€ src/
â”‚   â”œâ”€ ingestion/
â”‚   â”‚   â”œâ”€ data_ingestion.py
â”‚   â”‚   â””â”€ schema_discovery.py
â”‚   â”œâ”€ engines/
â”‚   â”‚   â”œâ”€ query_engine.py      # core planner/retriever/reasoner
â”‚   â”‚   â””â”€ cro_query_engine.py  # executive formatter & viz helper
â”‚   â”œâ”€ api_server.py            # FastAPI gateway
â”‚   â””â”€ components/              # visualisers, pydantic models, utils
â”œâ”€ data/                        # drop CSVs here
â”œâ”€ frontend/                    # React/Next.js UI
â””â”€ docs/
    â””â”€ query_engine.md          # inâ€‘depth engine architecture
```

---

## âš™ï¸Â Configuration

| Variable                                    | Purpose                |
| ------------------------------------------- | ---------------------- |
| `OPENAI_API_KEY`                            | LLM access             |
| `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` | Graph store            |
| `CHROMA_PATH`                               | Vector DB path         |
| `DATA_PATH`                                 | Folder containing CSVs |

All variables live in `.env` and are loaded by `pythonâ€‘dotenv`.

---

## ğŸ§ªÂ Testing

```bash
$ pytest tests/    # unit tests for ingestion & query planner
$ python tests/test_queries.py smoke   # endâ€‘toâ€‘end Q&A suite
```

---

## ğŸ› ï¸Â Extending

* **New data source?** Drop the file in `data/` and restart; the ingestion script rebuilds DuckDB tables, graph nodes, and embeddings.
* **Swap storage** layers (e.g., Snowflake for DuckDB) by editing `config.yaml`â€”the LLM prompt remains unchanged.
* **Custom tools** can be added to `ToolExecutor`; register in the Planner node to enable autoâ€‘selection.

