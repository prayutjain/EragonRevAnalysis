# Setup Guide

This guide walks you through getting the **Enterprise CRO Analytics System** running on your laptop, a Docker host, or the cloud. It covers both **backend** services and the **React/Next.js** frontend.

---

## 1Â Prerequisites

| Tool                    | Version | Notes                                 |
| ----------------------- | ------- | ------------------------------------- |
| Python                  | â‰¥â€¯3.12  | Use `pyenv` or system package manager |
| Node.js + npm           | â‰¥â€¯20    | Needed only for the frontend          |
| Docker & DockerÂ Compose | Latest  | Optional but recommended              |
| Neo4j                   | 5.x     | Desktop, Aura, or Docker image        |
| Git                     | any     | Clone the repo                        |

> **TipÂ (quickest path)** â€“ If you have Docker, skip to Â§â€¯4 (DockerÂ Compose). It spins up everythingâ€”including Neo4j and Chromaâ€”in one command.

---

## 2Â Local Development (no Docker)

\###Â 2.1Â Clone & Create VirtualÂ Env

```bash
$ git clone https://github.com/your-org/cro-analytics.git && cd cro-analytics
$ python -m venv .venv && source .venv/bin/activate
```

\###Â 2.2Â Install PythonÂ Deps

```bash
(.venv) $ pip install -r requirements.txt
```

\###Â 2.3Â Set Environment Variables

```bash
$ cp .env.example .env            # then edit with your keys
```

| Key                             | Description                              |
| ------------------------------- | ---------------------------------------- |
| `OPENAI_API_KEY`                | ChatCompletion access                    |
| `NEO4J_URI`                     | e.g. `bolt://localhost:7687`             |
| `NEO4J_USER`Â /Â `NEO4J_PASSWORD` | Neo4j creds                              |
| `CHROMA_PATH`                   | Folder where the vector DB will live     |
| `DATA_PATH`                     | Path to your raw CSVs (default `./data`) |

\###Â 2.4Â Start Databases

```bash
# Neo4j Desktop UI OR
$ neo4j start   # if installed via package

# Chroma runs inâ€‘process; no daemon needed
```

\###Â 2.5Â Ingest Data
The first boot will autoâ€‘run ingestion. To trigger manually:

```bash
(.venv) $ python src/ingestion/data_ingestion.py --fresh
```

You should now have DuckDB tables, a populated Neo4j graph, and Chroma embeddings.

\###Â 2.6Â Run the API

```bash
(.venv) $ python src/api_server.py    # default http://localhost:8083
```

`/docs` (FastAPI Swagger) should list endpoints like `/qa` and `/qa/stream`.

\###Â 2.7Â Start the Frontend

```bash
$ cd frontend
$ npm install            # first time only
$ cp .env.local.example .env.local   # set NEXT_PUBLIC_API_URL=http://localhost:8083
$ npm run dev            # http://localhost:3000
```

Ask the chatbot: *â€œWhat are the topÂ 5 accounts by total opportunity value?â€*

---

## 3Â Unit & Integration Tests

```bash
(.venv) $ pytest tests/            # ingestion + engine units
(.venv) $ python tests/test_queries.py smoke   # endâ€‘toâ€‘end Q&A
```

---

## 4Â DockerÂ Compose (Oneâ€‘liner)

```bash
$ docker compose up --build
```

*Services spun up:* `api` (FastAPI), `neo4j`, `chroma`, and `frontend` (Next.js). Visit `http://localhost:8083/docs` and `http://localhost:3000`.

\###Â 4.1Â Customising

* Override variables in `.env.docker`
* Mount a host `./data` folder via volume to ingest new CSVs automatically

---

## 5Â Production Deployment Sketch

| Layer    | Option                    | Notes                                         |
| -------- | ------------------------- | --------------------------------------------- |
| API      | AWSÂ ECSÂ /Â GKE             | Container image `cro-api:<tag>`               |
| Graph    | Neo4j Aura                | Scales to billions of nodes                   |
| Vectors  | Chroma Server or Pinecone | Pinecone dropâ€‘in via config.yaml              |
| Frontend | Vercel / CloudFront       | Build output `npm run build && npm run start` |
| CI/CD    | GitHubÂ Actions            | Lint â†’ Tests â†’ Build â†’ Push â†’ Deploy          |

*Encryption, secret management, and autoscaling policies should be set per your orgâ€™s standards.*

---

## 6Â Updating Data

1. Drop new CSV(s) into `data/`.
2. Restart the API or run `python src/ingestion/data_ingestion.py --fresh`.
3. The autoâ€‘discovery logic rebuilds DuckDB tables, reâ€‘syncs Neo4j, and reâ€‘embeds Chroma.

No schema handâ€‘holding required.

---

## 7Â Troubleshooting

| Symptom                       | Likely Cause                | Fix                                |
| ----------------------------- | --------------------------- | ---------------------------------- |
| `bolt://â€¦ connection refused` | Neo4j not running           | `neo4j start` or check Docker logs |
| `OpenAIAuthenticationError`   | Wrong API key               | Reâ€‘export `OPENAI_API_KEY`         |
| Queries return **empty**      | Ingestion skipped a file    | Check `DATA_PATH`, rerun ingestion |
| Frontend CORS errors          | API not on `localhost:8083` | Update `NEXT_PUBLIC_API_URL`       |

---

## 8Â Next Steps

* Configure SSL & SSO for enterprise rollâ€‘out.
* Swap DuckDB for Snowflake if datasets exceed local disk.
* Enable GPU embedding generation via `--device cuda` flag in `data_ingestion.py`.

Happy querying! ğŸ‰
